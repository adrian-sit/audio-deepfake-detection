{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a6ca146f-3bfe-414b-9c9d-a0ccf896c3e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /home/adriansit/miniconda3/lib/python3.12/site-packages (4.51.0)\n",
      "Requirement already satisfied: filelock in /home/adriansit/miniconda3/lib/python3.12/site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /home/adriansit/miniconda3/lib/python3.12/site-packages (from transformers) (0.30.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/adriansit/miniconda3/lib/python3.12/site-packages (from transformers) (2.1.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/adriansit/miniconda3/lib/python3.12/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/adriansit/miniconda3/lib/python3.12/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/adriansit/miniconda3/lib/python3.12/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /home/adriansit/miniconda3/lib/python3.12/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/adriansit/miniconda3/lib/python3.12/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/adriansit/miniconda3/lib/python3.12/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/adriansit/miniconda3/lib/python3.12/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/adriansit/miniconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/adriansit/miniconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/adriansit/miniconda3/lib/python3.12/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/adriansit/miniconda3/lib/python3.12/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/adriansit/miniconda3/lib/python3.12/site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/adriansit/miniconda3/lib/python3.12/site-packages (from requests->transformers) (2025.1.31)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install -U transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ee0e2c11-69c9-45f0-9b2b-8e7646c79c21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import models\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from transformers import Wav2Vec2Model, Wav2Vec2Processor\n",
    "import soundfile as sf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49319a77-6226-4ce5-b27c-90f494b95e43",
   "metadata": {},
   "source": [
    "# Load audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2232b793-bbde-4115-81a1-1e3919d14d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WaveformDataset(Dataset):\n",
    "    def __init__(self, loader_type):\n",
    "        \n",
    "        root = os.getcwd()\n",
    "        data_root = None\n",
    "        real_folder = None\n",
    "        fake_folder = None\n",
    "        \n",
    "        # get the folder\n",
    "        if loader_type == \"train\":\n",
    "            data_root = os.path.join(root, 'data/for-rerecorded/training')\n",
    "            real_folder = os.path.join(data_root, 'real')\n",
    "            fake_folder = os.path.join(data_root, 'fake')\n",
    "        elif loader_type == \"validation\":\n",
    "            data_root = os.path.join(root, 'data/for-rerecorded/validation')\n",
    "            real_folder = os.path.join(data_root, 'real')\n",
    "            fake_folder = os.path.join(data_root, 'fake')\n",
    "        elif loader_type == \"test\":\n",
    "            data_root = os.path.join(root, 'data/for-rerecorded/testing')\n",
    "            real_folder = os.path.join(data_root, 'real')\n",
    "            fake_folder = os.path.join(data_root, 'fake')\n",
    "        elif loader_type == \"ITWFull\":\n",
    "            data_root = os.path.join(root, 'data/release_in_the_wild/')\n",
    "            real_folder = os.path.join(data_root, 'real')\n",
    "            fake_folder = os.path.join(data_root, 'fake')\n",
    "        else:\n",
    "            # Should never occur.\n",
    "            pass\n",
    "        \n",
    "        self.real_files = []\n",
    "        self.fake_files = []\n",
    "        \n",
    "        # get real example filenames\n",
    "        suffix = f\".wav\"\n",
    "        for filename in os.listdir(real_folder):\n",
    "            # check if correct suffix and exists as a file\n",
    "            if filename.endswith(suffix) and os.path.isfile(os.path.join(real_folder, filename)):\n",
    "                this_filepath = os.path.join(real_folder, filename)\n",
    "                self.real_files.append(this_filepath)\n",
    "                \n",
    "        print(f\"Real examples for raw waveform {loader_type}: {len(self.real_files)}\")\n",
    "        \n",
    "        # get fake example filenames\n",
    "        suffix = f\".wav\"\n",
    "        for filename in os.listdir(fake_folder):\n",
    "            # check if correct suffix and exists as a file\n",
    "            if filename.endswith(suffix) and os.path.isfile(os.path.join(fake_folder, filename)):\n",
    "                this_filepath = os.path.join(fake_folder, filename)\n",
    "                self.fake_files.append(this_filepath)\n",
    "                \n",
    "        print(f\"Fake examples for raw waveform {loader_type}: {len(self.fake_files)}\")\n",
    "            \n",
    "        # load the raw waveform data\n",
    "        #\n",
    "        # References the dataloader from the RawNet implementation\n",
    "        \n",
    "        self.data = []\n",
    "        \n",
    "        \n",
    "        for real_file in self.real_files:\n",
    "            X, sr = sf.read(real_file)\n",
    "            X = X.astype(np.float64)\n",
    "            X = X.reshape(1, -1)\n",
    "            \n",
    "            fixed_length = 2 * sr\n",
    "            if X.shape[1] < fixed_length:\n",
    "                X[:, :fixed_length]\n",
    "            else:\n",
    "                pad_width = fixed_length - X.shape[1]\n",
    "                X = np.pad(X, ((0, 0), (0, pad_width)), mode='constant')\n",
    "                \n",
    "            # append\n",
    "            X = X.astype(np.float32)\n",
    "            self.data.append((X, 1))\n",
    "            \n",
    "        for fake_file in self.fake_files:\n",
    "            X, sr = sf.read(real_file)\n",
    "            X = X.astype(np.float64)\n",
    "            X = X.reshape(1, -1)\n",
    "\n",
    "            fixed_length = 2 * sr\n",
    "            if X.shape[1] < fixed_length:\n",
    "                X[:, :fixed_length]\n",
    "            else:\n",
    "                pad_width = fixed_length - X.shape[1]\n",
    "                X = np.pad(X, ((0, 0), (0, pad_width)), mode='constant')\n",
    "                \n",
    "            # append\n",
    "            X = X.astype(np.float32)\n",
    "            self.data.append((X, 0))\n",
    "            \n",
    "            \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "            \n",
    "    def __getitem__(self, idx):\n",
    "        # return the data and the label\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80cc2f0-d60a-4e86-911d-46e9c8450c29",
   "metadata": {},
   "source": [
    "# Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8609b1fd-6167-46e8-ae04-d93609a442d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Wav2VecClassifier(nn.Module):\n",
    "    def __init__(self, hidden_dim=768):\n",
    "        super().__init__()\n",
    "        self.wav2vec = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base-960h\", use_safetensors=False)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, 1)  \n",
    "        )\n",
    "\n",
    "    def forward(self, input_values, attention_mask):\n",
    "        outputs = self.wav2vec(input_values=input_values, attention_mask=attention_mask)\n",
    "        pooled = outputs.last_hidden_state.mean(dim=1)  \n",
    "        return self.classifier(pooled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b89ab53-1d6d-44f2-886b-8b8f5436300e",
   "metadata": {},
   "source": [
    "# Training Procedures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "93e4faf3-1a14-4a4b-801c-542a908c6fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "\n",
    "def collate_fn(batch):\n",
    "    waveforms, labels = zip(*batch)\n",
    "    waveforms = [torch.tensor(w[0], dtype=torch.float32) for w in waveforms]  \n",
    "    waveforms = [w.squeeze(0) for w in waveforms] \n",
    "    \n",
    "    inputs = processor(waveforms, sampling_rate=16000, return_tensors=\"pt\", padding=True)\n",
    "    labels = torch.tensor(labels, dtype=torch.float).unsqueeze(1)\n",
    "    return inputs, labels\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2ba055f5-5985-4222-9d2a-15bc1584deba",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'init_empty_weights' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m device = torch.device(\u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      3\u001b[39m model = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m model = \u001b[43mWav2VecClassifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m model.to(device)\n\u001b[32m      8\u001b[39m epochs = \u001b[32m5\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 4\u001b[39m, in \u001b[36mWav2VecClassifier.__init__\u001b[39m\u001b[34m(self, hidden_dim)\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_dim=\u001b[32m768\u001b[39m):\n\u001b[32m      3\u001b[39m     \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m()\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     \u001b[38;5;28mself\u001b[39m.wav2vec = \u001b[43mWav2Vec2Model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfacebook/wav2vec2-base-960h\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_safetensors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m     \u001b[38;5;28mself\u001b[39m.classifier = nn.Sequential(\n\u001b[32m      6\u001b[39m         nn.Linear(hidden_dim, \u001b[32m256\u001b[39m),\n\u001b[32m      7\u001b[39m         nn.ReLU(),\n\u001b[32m      8\u001b[39m         nn.Dropout(\u001b[32m0.2\u001b[39m),\n\u001b[32m      9\u001b[39m         nn.Linear(\u001b[32m256\u001b[39m, \u001b[32m1\u001b[39m)  \n\u001b[32m     10\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/transformers/modeling_utils.py:279\u001b[39m, in \u001b[36mrestore_default_torch_dtype.<locals>._wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    277\u001b[39m old_dtype = torch.get_default_dtype()\n\u001b[32m    278\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m279\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    280\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    281\u001b[39m     torch.set_default_dtype(old_dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/transformers/modeling_utils.py:4333\u001b[39m, in \u001b[36mPreTrainedModel.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[39m\n\u001b[32m   4330\u001b[39m config.name_or_path = pretrained_model_name_or_path\n\u001b[32m   4332\u001b[39m \u001b[38;5;66;03m# Instantiate model.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m4333\u001b[39m model_init_context = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_init_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43mis_quantized\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_is_ds_init_called\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4335\u001b[39m config = copy.deepcopy(config)  \u001b[38;5;66;03m# We do not want to modify the config inplace in from_pretrained.\u001b[39;00m\n\u001b[32m   4336\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(config, \u001b[33m\"\u001b[39m\u001b[33m_attn_implementation_autoset\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/transformers/modeling_utils.py:3736\u001b[39m, in \u001b[36mPreTrainedModel.get_init_context\u001b[39m\u001b[34m(cls, is_quantized, _is_ds_init_called)\u001b[39m\n\u001b[32m   3734\u001b[39m         init_contexts.append(set_quantized_state())\n\u001b[32m   3735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3736\u001b[39m     init_contexts = [no_init_weights(), \u001b[43minit_empty_weights\u001b[49m()]\n\u001b[32m   3738\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m init_contexts\n",
      "\u001b[31mNameError\u001b[39m: name 'init_empty_weights' is not defined"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "\n",
    "model = None\n",
    "model = Wav2VecClassifier()\n",
    "    \n",
    "model.to(device)\n",
    "\n",
    "epochs = 5\n",
    "batch_size = 32\n",
    "weight_decay = 5e-4\n",
    "learning_rate = 0.0001\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay = weight_decay)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max = epochs)\n",
    "\n",
    "FoR_train_dataset = WaveformDataset(\"train\")\n",
    "FoR_val_dataset = WaveformDataset(\"validation\")\n",
    "FoR_test_dataset = WaveformDataset(\"test\")\n",
    "FoR_train_loader = DataLoader(FoR_train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "FoR_val_loader = DataLoader(FoR_val_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "FoR_test_loader = DataLoader(FoR_test_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac5bd1e-049c-4113-b399-434bb18b24be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_EER(model, loader):\n",
    "    model.eval()\n",
    "    all_scores = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            waveform, labels = data\n",
    "            \n",
    "            waveform = waveform.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            out = model(waveform)\n",
    "            out = torch.sigmoid(out)\n",
    "\n",
    "            \n",
    "            all_scores.extend(out.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    # use sklearn to compute this for us\n",
    "    fpr, tpr, thresholds = roc_curve(all_labels, all_scores)\n",
    "    \n",
    "    # definition\n",
    "    fnr = 1 - tpr\n",
    "\n",
    "    # find closest threshold\n",
    "    eer_thresh = np.nanargmin(np.abs(fpr-fnr))\n",
    "    EER = (fpr[eer_thresh] + fnr[eer_thresh])/2\n",
    "    \n",
    "    return EER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48548bce-3974-4738-b293-fbfb801f758f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(loader):\n",
    "    model.train()\n",
    "    training_loss = 0.0\n",
    "    \n",
    "    for data in loader:\n",
    "        waveform, labels = data\n",
    "        waveform = waveform.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # basic pytorch boilerplate\n",
    "        out = model(waveform)\n",
    "        loss = criterion(out, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        training_loss += loss.item()\n",
    "        \n",
    "    training_loss = training_loss / len(loader)\n",
    "    return training_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ffe591-db4a-4d48-b2af-b0bd09e8bee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(loader):\n",
    "    model.eval()\n",
    "    validation_loss = 0.0\n",
    "    \n",
    "    n_correct = 0\n",
    "    n_total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            waveform, labels = data\n",
    "            \n",
    "            waveform = waveform.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            out = model(waveform)\n",
    "            loss = criterion(out, labels)\n",
    "            \n",
    "            validation_loss += loss.item()\n",
    "            \n",
    "            # count correct predictions\n",
    "            preds = None\n",
    "            preds = (out > 0).long()\n",
    "            \n",
    "            n_correct = n_correct + (preds == labels).sum().item()\n",
    "            n_total = n_total + labels.size(0)\n",
    "            \n",
    "    validation_loss = validation_loss / len(loader)\n",
    "    accuracy = n_correct / n_total\n",
    "    \n",
    "    return validation_loss, accuracy\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd35bff-24be-497d-8402-6a9df84f861f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference paper uses patience = 5\n",
    "patience = 5\n",
    "best_validation_loss = 10000.0\n",
    "fail_count = 0\n",
    "epochs = 30\n",
    "\n",
    "training_losses = []\n",
    "val_losses = []\n",
    "test_losses = []\n",
    "\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    training_loss = train(FoR_train_loader)\n",
    "    print(f\"[Epoch {epoch}] Training Loss: {training_loss}\")\n",
    "    \n",
    "    training_losses.append(training_loss)\n",
    "    \n",
    "    validation_loss, val_accuracy = validate(FoR_val_loader)    \n",
    "    print(f\"[Epoch {epoch}] Validation Loss: {validation_loss} Accuracy: {val_accuracy}\")\n",
    "    \n",
    "    val_losses.append(validation_loss)\n",
    "    \n",
    "    test_loss, test_accuracy = validate(FoR_test_loader)\n",
    "    print(f\"[DEBUG Epoch {epoch}] Test Loss: {test_loss} Accuracy: {test_accuracy}\")\n",
    "    \n",
    "    test_losses.append(test_loss)\n",
    "    \n",
    "    if validation_loss < best_validation_loss:\n",
    "        best_validation_loss = validation_loss\n",
    "        fail_count = 0\n",
    "    else:\n",
    "        # increment number of epochs of no improvement\n",
    "        fail_count = fail_count + 1\n",
    "        \n",
    "    if fail_count >= patience:\n",
    "        print(f\"Triggering early breaking on epoch {epoch}\")\n",
    "        break\n",
    "    \n",
    "    scheduler.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
