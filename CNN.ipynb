{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UEtR8P8MGhSX"
   },
   "source": [
    "# Processing for local training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "5S5KWJQSHn3z"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "#!tar -xf data/for-rerec.tar.gz -C data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8xbEPVigKihN",
    "outputId": "2aefed60-6275-444c-f190-753329d620b2"
   },
   "outputs": [],
   "source": [
    "#ls data/for-rerecorded/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QSWk1DLnIi_O",
    "outputId": "7747b4d1-d172-4189-d0fd-bc70149bf201"
   },
   "outputs": [],
   "source": [
    "# !pip3 install torch torchvision librosa matplotlib tqdm pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "8omdaYcFI6hU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import models\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import roc_curve\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aUWaXDsKNQ31"
   },
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "7wm4V3FxPjGC"
   },
   "outputs": [],
   "source": [
    "SAMPLE_RATE = 16000  # Sampling rate\n",
    "N_MELS = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "5DdJBRQqNS9C"
   },
   "outputs": [],
   "source": [
    "# TODO: Make it so each output is \"513-dimensional\" as with the reference paper\n",
    "#\n",
    "# https://arxiv.org/pdf/2203.16263\n",
    "\n",
    "def compute_spectrograms(path):\n",
    "    y, sr = librosa.load(path, sr=SAMPLE_RATE)\n",
    "    fixed_length = 2 * SAMPLE_RATE\n",
    "    if len(y) < fixed_length:\n",
    "        y = np.pad(y, (0, fixed_length - len(y)))\n",
    "    else:\n",
    "        y = y[:fixed_length]\n",
    "\n",
    "    cqt = librosa.cqt(y, sr=sr)\n",
    "    cqt_spec = librosa.amplitude_to_db(np.abs(cqt), ref=np.max)\n",
    "\n",
    "    stft = librosa.stft(y)\n",
    "    log_spec = librosa.amplitude_to_db(np.abs(stft), ref=np.max)\n",
    "\n",
    "    mel = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=N_MELS)\n",
    "    mel_spec = librosa.power_to_db(mel, ref=np.max)\n",
    "\n",
    "    return cqt_spec, log_spec, mel_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dirs = {\n",
    "    'training_fake': 'data/for-rerecorded/training/fake/',\n",
    "    'testing_fake': 'data/for-rerecorded/testing/fake/',\n",
    "    'validation_fake': 'data/for-rerecorded/validation/fake/',\n",
    "    'training_real': 'data/for-rerecorded/training/real/',\n",
    "    'testing_real': 'data/for-rerecorded/testing/real/',\n",
    "    'validation_real': 'data/for-rerecorded/validation/real/',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E6PzYHa5TqHT",
    "outputId": "c364cca2-105e-4aab-9763-0f2e211e8443"
   },
   "outputs": [],
   "source": [
    "def process_directory(directory, output_dir):\n",
    "  os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "  for filename in tqdm(os.listdir(directory)):\n",
    "    if filename.endswith('.wav'):\n",
    "      audio_path = os.path.join(directory, filename)\n",
    "      cqt, log, mel = compute_spectrograms(audio_path)\n",
    "\n",
    "      base_name = os.path.splitext(filename)[0]\n",
    "      # Save spectrograms as numpy arrays\n",
    "      np.save(f\"{output_dir}/{base_name}_cqt.npy\", cqt)\n",
    "      np.save(f\"{output_dir}/{base_name}_log.npy\", log)\n",
    "      np.save(f\"{output_dir}/{base_name}_mel.npy\", mel)\n",
    "\n",
    "compute_specs = False\n",
    "if compute_specs:\n",
    "    for set_name, directory in data_dirs.items():\n",
    "      output_dir = f'data/spectrograms/{set_name}_spectrograms'\n",
    "      process_directory(directory, output_dir)\n",
    "      print(f\"Processed {set_name} set.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(84, 63)\n",
      "(1025, 63)\n",
      "(128, 63)\n"
     ]
    }
   ],
   "source": [
    "# lets look at some data\n",
    "display_cqt = \"data/spectrograms/training_fake_spectrograms/recording1.wav_norm_mono_cqt.npy\"\n",
    "display_log = \"data/spectrograms/training_fake_spectrograms/recording1.wav_norm_mono_log.npy\"\n",
    "display_mel = \"data/spectrograms/training_fake_spectrograms/recording1.wav_norm_mono_mel.npy\"\n",
    "\n",
    "cqt_test = np.load(display_cqt)\n",
    "log_test = np.load(display_log)\n",
    "mel_test = np.load(display_mel)\n",
    "print(cqt_test.shape)\n",
    "print(log_test.shape)\n",
    "print(mel_test.shape)\n",
    "\n",
    "# for reference\n",
    "cqt_size = 84\n",
    "log_size = 1025\n",
    "mel_size = 128\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k-DJw8fFNZXj"
   },
   "source": [
    "# Define models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "S2eATQYGNaUY"
   },
   "outputs": [],
   "source": [
    "class ResNet50Spectrogram(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResNet50Spectrogram, self).__init__()\n",
    "\n",
    "        self.model = models.resnet50(weights=None)\n",
    "\n",
    "        original_conv = self.model.conv1\n",
    "        self.model.conv1 = nn.Conv2d(in_channels=1,\n",
    "                            out_channels=original_conv.out_channels,\n",
    "                            kernel_size = original_conv.kernel_size,\n",
    "                            stride = original_conv.stride,\n",
    "                            padding = original_conv.padding,\n",
    "                            bias = False)\n",
    "\n",
    "        self.model.fc = nn.Linear(self.model.fc.in_features, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EfficientNetSpectrogram(nn.Module):\n",
    "    def __init__(self, model_type):\n",
    "        super(EfficientNetSpectrogram, self).__init__()\n",
    "        \n",
    "        self.enet = None\n",
    "        \n",
    "        if model_type == \"b0\":\n",
    "            self.enet = models.efficientnet_b0(weights=None, num_classes=2)\n",
    "            \n",
    "        \n",
    "        # We need to change the network to accept 1 channel instead of\n",
    "        # 3 because of our data.\n",
    "        original_conv = self.enet.features[0][0]\n",
    "        new_conv = nn.Conv2d(in_channels=1,\n",
    "                            out_channels=original_conv.out_channels,\n",
    "                            kernel_size = original_conv.kernel_size,\n",
    "                            stride = original_conv.stride,\n",
    "                            padding = original_conv.padding,\n",
    "                            bias = False)\n",
    "        self.enet.features[0][0] = new_conv\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.enet(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMSpectrogram(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LSTMSpectrogram, self).__init__()\n",
    "        \n",
    "        self.nlayer = 2\n",
    "        self.nhiddens = 256\n",
    "        \n",
    "        if feature_type == \"cqt\":\n",
    "            self.lstm = nn.LSTM(input_size=cqt_size, hidden_size=self.nhiddens, num_layers=self.nlayer, \n",
    "                                batch_first=True, dropout=0.3)\n",
    "        elif feature_type == \"log\":\n",
    "            self.lstm = nn.LSTM(input_size=log_size, hidden_size=self.nhiddens, num_layers=self.nlayer, \n",
    "                                batch_first=True, dropout=0.3)\n",
    "        elif feature_type == \"mel\":\n",
    "            self.lstm = nn.LSTM(input_size=mel_size, hidden_size=self.nhiddens, num_layers=self.nlayer, \n",
    "                                batch_first=True, dropout=0.3)\n",
    "            \n",
    "        self.fc = nn.Linear(self.nhiddens, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.squeeze(1)\n",
    "        # features are in wrong order for lstm\n",
    "        x = x.transpose(1,2)\n",
    "        \n",
    "        x, (h_o, c_o) = self.lstm(x)\n",
    "        \n",
    "        h_o = h_o.squeeze(0)\n",
    "        if self.nlayer > 1:\n",
    "            h_o = h_o[-1]\n",
    "        x = self.fc(h_o)\n",
    "        x = x.squeeze(-1)\n",
    "        \n",
    "        #x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training procedures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpecDataset(Dataset):\n",
    "    # data_type is one of 'cqt', 'log', 'mel'\n",
    "    #\n",
    "    # loader_type is one of 'train', 'validation', 'test'\n",
    "    def __init__(self, data_type, loader_type):\n",
    "        \n",
    "        root = os.getcwd()\n",
    "        data_root = os.path.join(root, 'data/spectrograms')\n",
    "        \n",
    "        self.data = []\n",
    "        \n",
    "        real_folder = None\n",
    "        fake_folder = None\n",
    "\n",
    "        # get the folder\n",
    "        if loader_type == \"train\":\n",
    "            real_folder = os.path.join(data_root, 'training_real_spectrograms')\n",
    "            fake_folder = os.path.join(data_root, 'training_fake_spectrograms')\n",
    "        elif loader_type == \"validation\":\n",
    "            real_folder = os.path.join(data_root, 'validation_real_spectrograms')\n",
    "            fake_folder = os.path.join(data_root, 'validation_fake_spectrograms')\n",
    "        elif loader_type == \"test\":\n",
    "            real_folder = os.path.join(data_root, 'testing_real_spectrograms')\n",
    "            fake_folder = os.path.join(data_root, 'testing_fake_spectrograms')\n",
    "        elif loader_type == \"ITWFull\":\n",
    "            real_folder = os.path.join(data_root, 'ITWfull_real_spectrograms')\n",
    "            fake_folder = os.path.join(data_root, 'ITWfull_fake_spectrograms')\n",
    "        else:\n",
    "            # Should never occur.\n",
    "            pass\n",
    "        \n",
    "        real_files = []\n",
    "        fake_files = []\n",
    "        \n",
    "        # now we have the folder given the loader type, collect\n",
    "        # the data required for the loader.\n",
    "        \n",
    "        # get real example filenames\n",
    "        suffix = f\"{data_type}.npy\"\n",
    "        for filename in os.listdir(real_folder):\n",
    "            # check if correct suffix and exists as a file\n",
    "            if filename.endswith(suffix) and os.path.isfile(os.path.join(real_folder, filename)):\n",
    "                this_filepath = os.path.join(real_folder, filename)\n",
    "                real_files.append(this_filepath)\n",
    "                \n",
    "        print(f\"Real examples for {data_type} {loader_type}: {len(real_files)}\")\n",
    "        \n",
    "        # get fake example filenames\n",
    "        suffix = f\"{data_type}.npy\"\n",
    "        for filename in os.listdir(fake_folder):\n",
    "            # check if correct suffix and exists as a file\n",
    "            if filename.endswith(suffix) and os.path.isfile(os.path.join(fake_folder, filename)):\n",
    "                this_filepath = os.path.join(fake_folder, filename)\n",
    "                fake_files.append(this_filepath)\n",
    "                \n",
    "        print(f\"Fake examples for {data_type} {loader_type}: {len(fake_files)}\")\n",
    "        \n",
    "        label_val_false = 0\n",
    "        label_val_true = 1\n",
    "        if model_type == \"LSTM\":\n",
    "            label_val_false = float(0)\n",
    "            label_val_true = float(1)\n",
    "        \n",
    "        # load the data into memory\n",
    "        #\n",
    "        # if we need to work with a larger dataset, you might need to\n",
    "        # alter this to be lazy loading instead, but it fits in my main memory\n",
    "        # because of how much I currently have.\n",
    "        for real_file in real_files:\n",
    "            rf_data = torch.tensor(np.load(real_file))\n",
    "            rf_data = rf_data.unsqueeze(0)\n",
    "            if resizing == True:\n",
    "                rf_data = rf_data.unsqueeze(0)\n",
    "                rf_data = F.interpolate(rf_data, size=dims_resize, mode='bilinear', align_corners = False)\n",
    "                rf_data = rf_data.squeeze(0)\n",
    "            self.data.append((rf_data,label_val_true))\n",
    "            \n",
    "        for fake_file in fake_files:\n",
    "            ff_data = torch.tensor(np.load(fake_file))\n",
    "            ff_data = ff_data.unsqueeze(0)\n",
    "            if resizing == True:\n",
    "                ff_data = ff_data.unsqueeze(0)\n",
    "                ff_data = F.interpolate(ff_data, size=dims_resize, mode='bilinear', align_corners = False)\n",
    "                ff_data = ff_data.squeeze(0)\n",
    "            self.data.append((ff_data, label_val_false))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "            \n",
    "    def __getitem__(self, idx):\n",
    "        # return the data and the label\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dynamic_collate(batch):\n",
    "    data, labels = zip(*batch)\n",
    "    data = [d for d in data]\n",
    "    \n",
    "    max_length = max(d.shape[2] for d in data)\n",
    "    \n",
    "    padded = []\n",
    "    if resizing == False:\n",
    "        for d in data:\n",
    "            # total padding needed, >= 0\n",
    "            padding = max_length - d.shape[2]\n",
    "        \n",
    "            padded_d = None\n",
    "            if padding > 0:\n",
    "                # add zero's (silence) to match rest of batch\n",
    "                padded_data = F.pad(d, (0,padding))\n",
    "            \n",
    "            else:\n",
    "                # already max length\n",
    "                padded_data = d\n",
    "            padded.append(padded_data)\n",
    "    else:\n",
    "        # if resizing was true, we don't need to pad, everything is of the same shape\n",
    "        padded = data\n",
    "    \n",
    "    '''\n",
    "    for p in padded:\n",
    "        r = p.unsqueeze(0)\n",
    "        r = F.interpolate(r, size=dims_resize, mode='bilinear', align_corners = False)\n",
    "        r = r.squeeze(0)\n",
    "        resized.append(r)\n",
    "    '''\n",
    "    \n",
    "    # stack properly now that everything is padded\n",
    "    padded = torch.stack(padded, dim=0)\n",
    "\n",
    "    labels = torch.tensor(labels)\n",
    "    \n",
    "    return padded, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real examples for log train: 5104\n",
      "Fake examples for log train: 5104\n",
      "Real examples for log validation: 1101\n",
      "Fake examples for log validation: 1143\n",
      "Real examples for log test: 408\n",
      "Fake examples for log test: 408\n"
     ]
    }
   ],
   "source": [
    "#mean = [0]\n",
    "#std = [1]\n",
    "\n",
    "# deal with this later\n",
    "#\n",
    "# we should also probably compute the mean and std manually instead of assuming they correctly\n",
    "# normalized it, since this is the re-recorded dataset\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "    #transforms.Normalize(mean, std)\n",
    "  ])\n",
    "test_transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "####################################################\n",
    "# <CHANGE ME> if you want to use different features!\n",
    "####################################################\n",
    "feature_type = \"log\"\n",
    "\n",
    "####################################################\n",
    "# <CHANGE ME> if you want to use resizing!\n",
    "#\n",
    "# We need to resize to, for example, (224, 224)\n",
    "####################################################\n",
    "resizing = True\n",
    "dims_resize = (224, 224)\n",
    "\n",
    "#model_type = \"enet\"\n",
    "model_type = \"res\"\n",
    "#model_type = \"LSTM\"\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "model = None\n",
    "\n",
    "if model_type == \"LSTM\":\n",
    "    model = LSTMSpectrogram()\n",
    "elif model_type == \"enet\":\n",
    "    model =  EfficientNetSpectrogram(\"b0\")\n",
    "elif model_type == \"res\":\n",
    "    model = ResNet50Spectrogram()\n",
    "    \n",
    "model = model.to(device)\n",
    "\n",
    "#epochs = 100\n",
    "epochs = 30\n",
    "batch_size = 32\n",
    "weight_decay = 5e-4\n",
    "learning_rate = 0.0001\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "if model_type == \"LSTM\":\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay = weight_decay)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max = epochs)\n",
    "\n",
    "FoR_train_loader = None\n",
    "FoR_val_loader = None\n",
    "FoR_test_loader = None\n",
    "\n",
    "# data loaders\n",
    "FoR_train_dataset = SpecDataset(feature_type, \"train\")\n",
    "FoR_val_dataset = SpecDataset(feature_type, \"validation\")\n",
    "FoR_test_dataset = SpecDataset(feature_type, \"test\")\n",
    "\n",
    "FoR_train_loader = DataLoader(FoR_train_dataset, batch_size=batch_size, shuffle=True, collate_fn=dynamic_collate)\n",
    "FoR_val_loader = DataLoader(FoR_val_dataset, batch_size=batch_size, shuffle=True, collate_fn=dynamic_collate)\n",
    "FoR_test_loader = DataLoader(FoR_test_dataset, batch_size=batch_size, shuffle=True, collate_fn=dynamic_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to compute the equal error rate as one of our metrics.\n",
    "def compute_EER(model, loader):\n",
    "    model.eval()\n",
    "    all_scores = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            waveform, labels = data\n",
    "            \n",
    "            waveform = waveform.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            out = model(waveform)\n",
    "            if model_type == \"LSTM\":\n",
    "                out = torch.sigmoid(out)\n",
    "            else:\n",
    "                out = torch.softmax(out, dim=1)\n",
    "                # take the positive class labels\n",
    "                out = out[:,1]\n",
    "            \n",
    "            all_scores.extend(out.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    # use sklearn to compute this for us\n",
    "    fpr, tpr, thresholds = roc_curve(all_labels, all_scores)\n",
    "    \n",
    "    # definition\n",
    "    fnr = 1 - tpr\n",
    "\n",
    "    # find closest threshold\n",
    "    eer_thresh = np.nanargmin(np.abs(fpr-fnr))\n",
    "    EER = (fpr[eer_thresh] + fnr[eer_thresh])/2\n",
    "    \n",
    "    return EER\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(loader):\n",
    "    model.train()\n",
    "    training_loss = 0.0\n",
    "    \n",
    "    for data in loader:\n",
    "        waveform, labels = data\n",
    "        waveform = waveform.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # basic pytorch boilerplate\n",
    "        out = model(waveform)\n",
    "        loss = criterion(out, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        training_loss += loss.item()\n",
    "        \n",
    "    training_loss = training_loss / len(loader)\n",
    "    return training_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(loader):\n",
    "    model.eval()\n",
    "    validation_loss = 0.0\n",
    "    \n",
    "    n_correct = 0\n",
    "    n_total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            waveform, labels = data\n",
    "            \n",
    "            waveform = waveform.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            out = model(waveform)\n",
    "            loss = criterion(out, labels)\n",
    "            \n",
    "            validation_loss += loss.item()\n",
    "            \n",
    "            # count correct predictions\n",
    "            preds = None\n",
    "            if model_type == \"LSTM\":\n",
    "                preds = (out > 0).long()\n",
    "            else:\n",
    "                preds = out.argmax(dim=1)\n",
    "            \n",
    "            n_correct = n_correct + (preds == labels).sum().item()\n",
    "            n_total = n_total + labels.size(0)\n",
    "            \n",
    "    validation_loss = validation_loss / len(loader)\n",
    "    accuracy = n_correct / n_total\n",
    "    \n",
    "    return validation_loss, accuracy\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                            | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0] Training Loss: 0.37124145485250554\n",
      "[Epoch 0] Validation Loss: 0.2722762826252991 Accuracy: 0.8863636363636364\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|██▋                                                                              | 1/30 [02:32<1:13:51, 152.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG Epoch 0] Test Loss: 0.7090642612714034 Accuracy: 0.7022058823529411\n",
      "[Epoch 1] Training Loss: 0.1612746174385929\n",
      "[Epoch 1] Validation Loss: 0.15724595247859685 Accuracy: 0.9407308377896613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|█████▍                                                                           | 2/30 [09:07<2:17:44, 295.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG Epoch 1] Test Loss: 1.041413746201075 Accuracy: 0.6593137254901961\n",
      "[Epoch 2] Training Loss: 0.09736738857910983\n",
      "[Epoch 2] Validation Loss: 0.12890241352487092 Accuracy: 0.9527629233511586\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|████████                                                                         | 3/30 [16:07<2:38:21, 351.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG Epoch 2] Test Loss: 1.3922355610590715 Accuracy: 0.6176470588235294\n",
      "[Epoch 3] Training Loss: 0.06983439575991707\n",
      "[Epoch 3] Validation Loss: 0.09815628299305976 Accuracy: 0.966131907308378\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|██████████▊                                                                      | 4/30 [23:01<2:43:10, 376.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG Epoch 3] Test Loss: 0.8845844601209347 Accuracy: 0.7034313725490197\n",
      "[Epoch 4] Training Loss: 0.055230470140301115\n",
      "[Epoch 4] Validation Loss: 0.1065909654118488 Accuracy: 0.9710338680926917\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█████████████▌                                                                   | 5/30 [30:34<2:48:22, 404.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG Epoch 4] Test Loss: 1.5850116381278405 Accuracy: 0.6740196078431373\n",
      "[Epoch 5] Training Loss: 0.03968578453710723\n",
      "[Epoch 5] Validation Loss: 0.2723827622527532 Accuracy: 0.9327094474153298\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|████████████████▏                                                                | 6/30 [38:41<2:52:56, 432.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG Epoch 5] Test Loss: 1.5275486478438745 Accuracy: 0.6495098039215687\n",
      "[Epoch 6] Training Loss: 0.04241336406955604\n",
      "[Epoch 6] Validation Loss: 0.07857141210238489 Accuracy: 0.9754901960784313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██████████████████▉                                                              | 7/30 [45:58<2:46:16, 433.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG Epoch 6] Test Loss: 2.046506418631627 Accuracy: 0.6066176470588235\n",
      "[Epoch 7] Training Loss: 0.033735197724044236\n",
      "[Epoch 7] Validation Loss: 0.38053533442082327 Accuracy: 0.8832442067736186\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|█████████████████████▌                                                           | 8/30 [55:17<2:53:42, 473.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG Epoch 7] Test Loss: 4.5555591858350315 Accuracy: 0.5147058823529411\n",
      "[Epoch 8] Training Loss: 0.030094338735152434\n",
      "[Epoch 8] Validation Loss: 0.10988308090190271 Accuracy: 0.9674688057040999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███████████████████████▋                                                       | 9/30 [1:03:28<2:47:41, 479.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG Epoch 8] Test Loss: 2.5107709169387817 Accuracy: 0.5882352941176471\n",
      "[Epoch 9] Training Loss: 0.020042851171606816\n",
      "[Epoch 9] Validation Loss: 0.08330403118400002 Accuracy: 0.9781639928698752\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|██████████████████████████                                                    | 10/30 [1:10:31<2:33:58, 461.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG Epoch 9] Test Loss: 1.203284339262889 Accuracy: 0.7279411764705882\n",
      "[Epoch 10] Training Loss: 0.020739207701395077\n",
      "[Epoch 10] Validation Loss: 0.08406051221211616 Accuracy: 0.9737076648841355\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|████████████████████████████▌                                                 | 11/30 [1:17:37<2:22:47, 450.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG Epoch 10] Test Loss: 1.7863023716669817 Accuracy: 0.6482843137254902\n",
      "[Epoch 11] Training Loss: 0.00913654400017629\n",
      "[Epoch 11] Validation Loss: 0.0915693430822141 Accuracy: 0.9763814616755794\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|████████████████████████████▌                                                 | 11/30 [1:25:47<2:28:10, 467.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG Epoch 11] Test Loss: 2.7673317331534166 Accuracy: 0.633578431372549\n",
      "Triggering early breaking on epoch 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# reference paper uses patience = 5\n",
    "patience = 5\n",
    "best_validation_loss = 10000.0\n",
    "fail_count = 0\n",
    "epochs = 30\n",
    "\n",
    "training_losses = []\n",
    "val_losses = []\n",
    "test_losses = []\n",
    "\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    training_loss = train(FoR_train_loader)\n",
    "    print(f\"[Epoch {epoch}] Training Loss: {training_loss}\")\n",
    "    \n",
    "    training_losses.append(training_loss)\n",
    "    \n",
    "    validation_loss, val_accuracy = validate(FoR_val_loader)    \n",
    "    print(f\"[Epoch {epoch}] Validation Loss: {validation_loss} Accuracy: {val_accuracy}\")\n",
    "    \n",
    "    val_losses.append(validation_loss)\n",
    "    \n",
    "    test_loss, test_accuracy = validate(FoR_test_loader)\n",
    "    print(f\"[DEBUG Epoch {epoch}] Test Loss: {test_loss} Accuracy: {test_accuracy}\")\n",
    "    \n",
    "    test_losses.append(test_loss)\n",
    "    \n",
    "    if validation_loss < best_validation_loss:\n",
    "        best_validation_loss = validation_loss\n",
    "        fail_count = 0\n",
    "    else:\n",
    "        # increment number of epochs of no improvement\n",
    "        fail_count = fail_count + 1\n",
    "        \n",
    "    if fail_count >= patience:\n",
    "        print(f\"Triggering early breaking on epoch {epoch}\")\n",
    "        break\n",
    "    \n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test EER: 0.29289215686274506\n",
      "Testing loss: 2.7867565155029297 Accuracy: 0.633578431372549\n",
      "Train EER: 0.0007836990595611167\n"
     ]
    }
   ],
   "source": [
    "print(f\"Test EER: {compute_EER(model, FoR_test_loader)}\")\n",
    "test_loss, test_accuracy = validate(FoR_test_loader)\n",
    "print(f\"Testing loss: {test_loss} Accuracy: {test_accuracy}\")\n",
    "\n",
    "# expected to be quite low, though obvious overfitting at current settings\n",
    "print(f\"Train EER: {compute_EER(model, FoR_train_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(training_losses)), training_losses, label=\"Training Loss\", marker='o')\n",
    "plt.plot(range(len(val_losses)), val_losses, label=\"Validation Loss\", marker='s')\n",
    "plt.plot(range(len(test_losses)), test_losses, label=\"Test Loss\", marker='x')\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss vs epochs')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'models/ResNet_log.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "model = ResNet50Spectrogram()\n",
    "model.load_state_dict(torch.load('models/ResNet_cqt.pth'))\n",
    "model.eval()\n",
    "model = model.to(device)\n",
    "model_type = \"res\"\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process entire ITW dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(\"data/release_in_the_wild\"):\n",
    "    with zipfile.ZipFile('data/release_in_the_wild.zip') as zip_ref:\n",
    "        zip_ref.extractall('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_csv = 'data/release_in_the_wild/meta.csv'\n",
    "\n",
    "df = pd.read_csv(src_csv)\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    name = row['file']\n",
    "    label = str(row['label'])\n",
    "\n",
    "    src_path = os.path.join('data/release_in_the_wild', name)\n",
    "    dst_dir = os.path.join('data/release_in_the_wild', label)\n",
    "    dst_path = os.path.join(dst_dir, name)\n",
    "\n",
    "    os.makedirs(dst_dir, exist_ok=True)\n",
    "\n",
    "    if os.path.exists(src_path):\n",
    "        shutil.move(src_path, dst_path)\n",
    "\n",
    "os.rename('data/release_in_the_wild/bona-fide', 'data/release_in_the_wild/real')\n",
    "os.rename('data/release_in_the_wild/spoof', 'data/release_in_the_wild/fake')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_directory(directory, output_dir):\n",
    "  os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "  for filename in os.listdir(directory):\n",
    "    if filename.endswith('.wav'):\n",
    "      audio_path = os.path.join(directory, filename)\n",
    "      cqt, log, mel = compute_spectrograms(audio_path)\n",
    "\n",
    "      base_name = os.path.splitext(filename)[0]\n",
    "      # Save spectrograms as numpy arrays\n",
    "      np.save(f\"{output_dir}/{base_name}_cqt.npy\", cqt)\n",
    "      np.save(f\"{output_dir}/{base_name}_log.npy\", log)\n",
    "      np.save(f\"{output_dir}/{base_name}_mel.npy\", mel)\n",
    "\n",
    "data_dirs = {\n",
    "    'ITWfull_real': 'data/release_in_the_wild/real',\n",
    "    'ITWfull_fake': 'data/release_in_the_wild/fake/'\n",
    "}\n",
    "if compute_specs:\n",
    "    for set_name, directory in data_dirs.items():\n",
    "        output_dir = f'data/spectrograms/{set_name}_spectrograms'\n",
    "        process_directory(directory, output_dir)\n",
    "        print(f\"Processed {set_name} set.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(84, 63)\n",
      "(1025, 63)\n",
      "(128, 63)\n"
     ]
    }
   ],
   "source": [
    "# viewing some of the data\n",
    "ITW_display_cqt = \"data/spectrograms/ITWfull_real_spectrograms/5_cqt.npy\"\n",
    "ITW_display_log = \"data/spectrograms/ITWfull_real_spectrograms/5_log.npy\"\n",
    "ITW_display_mel = \"data/spectrograms/ITWfull_real_spectrograms/5_mel.npy\"\n",
    "\n",
    "ITW_cqt_test = np.load(display_cqt)\n",
    "ITW_log_test = np.load(display_log)\n",
    "ITW_mel_test = np.load(display_mel)\n",
    "print(ITW_cqt_test.shape)\n",
    "print(ITW_log_test.shape)\n",
    "print(ITW_mel_test.shape)\n",
    "\n",
    "# for reference, should be the same as before\n",
    "ITW_cqt_size = 84\n",
    "ITW_log_size = 1025\n",
    "ITW_mel_size = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the FoR trained model on the ITW dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real examples for log ITWFull: 19963\n",
      "Fake examples for log ITWFull: 11816\n"
     ]
    }
   ],
   "source": [
    "# already defined above\n",
    "feature_type = \"log\"\n",
    "resizing = True\n",
    "dims_resize = (224, 224)\n",
    "model_type = \"res\"\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "ITW_full_dataset = SpecDataset(feature_type, \"ITWFull\")\n",
    "ITW_full_loader = DataLoader(ITW_full_dataset, batch_size=batch_size, shuffle=True, collate_fn=dynamic_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITW Test EER: 0.29202589660540246\n",
      "ITW Testing loss: 2.3529806414761216 Accuracy: 0.6387236854526575\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "print(f\"ITW Test EER: {compute_EER(model, ITW_full_loader)}\")\n",
    "test_loss, test_accuracy = validate(ITW_full_loader)\n",
    "print(f\"ITW Testing loss: {test_loss} Accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer learning from FoR to ITW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25423\n",
      "3177\n",
      "3179\n"
     ]
    }
   ],
   "source": [
    "# Most of the settings should be kept the same from previous training, because we are\n",
    "# using the same model.\n",
    "\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "train_size = 0.8 * len(ITW_full_dataset)\n",
    "val_size = 0.1 * len(ITW_full_dataset)\n",
    "train_size = int(train_size)\n",
    "val_size = int(val_size)\n",
    "\n",
    "test_size = len(ITW_full_dataset) - val_size - train_size\n",
    "\n",
    "# now they should all sum to ITW_full_dataset, do the split\n",
    "\n",
    "ITW_train, ITW_val, ITW_test = random_split(ITW_full_dataset, [train_size, val_size, test_size])\n",
    "\n",
    "batch_size = 32\n",
    "epochs = 30\n",
    "weight_decay = 5e-4\n",
    "learning_rate = 0.0001\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay = weight_decay)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max = epochs)\n",
    "\n",
    "ITW_train_loader = DataLoader(ITW_train, batch_size=batch_size, shuffle=True, collate_fn=dynamic_collate)\n",
    "ITW_val_loader = DataLoader(ITW_val, batch_size=batch_size, shuffle=True, collate_fn=dynamic_collate)\n",
    "ITW_test_loader = DataLoader(ITW_test, batch_size=batch_size, shuffle=True, collate_fn=dynamic_collate)\n",
    "\n",
    "print(len(ITW_train_loader.dataset))\n",
    "print(len(ITW_val_loader.dataset))\n",
    "print(len(ITW_test_loader.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting transfer learning from FoR dataset model to ITW\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                            | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0] Training Loss: 0.05553503832637966\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|██▋                                                                              | 1/30 [16:13<7:50:30, 973.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0] Validation Loss: 0.07384771597862709 Accuracy: 0.9719861504564055\n",
      "[Epoch 1] Training Loss: 0.017394461650883673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|█████▎                                                                          | 2/30 [34:06<8:01:29, 1031.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] Validation Loss: 0.03014316294043965 Accuracy: 0.9911866540761725\n",
      "[Epoch 2] Training Loss: 0.012930405923978457\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|████████                                                                        | 3/30 [52:12<7:55:32, 1056.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2] Validation Loss: 0.008364373966687709 Accuracy: 0.9971671388101983\n",
      "[Epoch 3] Training Loss: 0.015296023972846828\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|██████████▍                                                                   | 4/30 [1:10:16<7:42:38, 1067.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3] Validation Loss: 0.010723048777144868 Accuracy: 0.996222851746931\n",
      "[Epoch 4] Training Loss: 0.009140315692710479\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█████████████                                                                 | 5/30 [1:27:14<7:17:21, 1049.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 4] Validation Loss: 0.007654492402562028 Accuracy: 0.9971671388101983\n",
      "[Epoch 5] Training Loss: 0.008191908756438604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|███████████████▌                                                              | 6/30 [1:44:51<7:00:47, 1051.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 5] Validation Loss: 0.0064674300309980025 Accuracy: 0.9971671388101983\n",
      "[Epoch 6] Training Loss: 0.005799241441025847\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██████████████████▏                                                           | 7/30 [2:01:12<6:34:27, 1029.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 6] Validation Loss: 0.009235581871926114 Accuracy: 0.9965376141013534\n",
      "[Epoch 7] Training Loss: 0.005593754292278888\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|████████████████████▊                                                         | 8/30 [2:17:29<6:11:12, 1012.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 7] Validation Loss: 0.004259973008056477 Accuracy: 0.9981114258734656\n",
      "[Epoch 8] Training Loss: 0.0035830006815761655\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███████████████████████▍                                                      | 9/30 [2:35:09<5:59:31, 1027.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 8] Validation Loss: 0.0043986171832189665 Accuracy: 0.998426188227888\n",
      "[Epoch 9] Training Loss: 0.00430395858285956\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|█████████████████████████▋                                                   | 10/30 [2:52:18<5:42:37, 1027.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 9] Validation Loss: 0.009180667863711278 Accuracy: 0.9971671388101983\n",
      "[Epoch 10] Training Loss: 0.0022808275932298503\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|████████████████████████████▏                                                | 11/30 [3:08:41<5:21:07, 1014.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 10] Validation Loss: 0.08432432807225268 Accuracy: 0.9748190116462071\n",
      "[Epoch 11] Training Loss: 0.0029864266350486356\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|███████████████████████████████▏                                              | 12/30 [3:24:41<4:59:15, 997.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 11] Validation Loss: 0.005726329073756915 Accuracy: 0.998426188227888\n",
      "[Epoch 12] Training Loss: 0.002697717660196529\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|██████████████████████████████▊                                              | 12/30 [3:40:43<5:31:05, 1103.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 12] Validation Loss: 0.007115433502077622 Accuracy: 0.9971671388101983\n",
      "Triggering early breaking on epoch 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Same loop, but for our ITW transfer learning.\n",
    "\n",
    "patience = 5\n",
    "best_validation_loss = 10000.0\n",
    "fail_count = 0\n",
    "\n",
    "TL_training_losses = []\n",
    "TL_val_losses = []\n",
    "# TL_test_losses = []\n",
    "\n",
    "print(\"Starting transfer learning from FoR dataset model to ITW\")\n",
    "\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    training_loss = train(ITW_train_loader)\n",
    "    print(f\"[Epoch {epoch}] Training Loss: {training_loss}\")\n",
    "    \n",
    "    TL_training_losses.append(training_loss)\n",
    "    \n",
    "    validation_loss, val_accuracy = validate(ITW_val_loader)    \n",
    "    print(f\"[Epoch {epoch}] Validation Loss: {validation_loss} Accuracy: {val_accuracy}\")\n",
    "    \n",
    "    TL_val_losses.append(validation_loss)\n",
    "    \n",
    "    # test_loss, test_accuracy = validate(ITW_test_loader)\n",
    "    # print(f\"[DEBUG Epoch {epoch}] Test Loss: {test_loss} Accuracy: {test_accuracy}\")\n",
    "    \n",
    "    # TL_test_losses.append(test_loss)\n",
    "    \n",
    "    if validation_loss < best_validation_loss:\n",
    "        best_validation_loss = validation_loss\n",
    "        fail_count = 0\n",
    "    else:\n",
    "        # increment number of epochs of no improvement\n",
    "        fail_count = fail_count + 1\n",
    "        \n",
    "    if fail_count >= patience:\n",
    "        print(f\"Triggering early breaking on epoch {epoch}\")\n",
    "        break\n",
    "    \n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transfer Learning ITW Test EER: 0.004360804863950296\n",
      "Testing loss: 0.009879228580221024 Accuracy: 0.9977980497011639\n"
     ]
    }
   ],
   "source": [
    "print(f\"Transfer Learning ITW Test EER: {compute_EER(model, ITW_test_loader)}\")\n",
    "test_loss, test_accuracy = validate(ITW_test_loader)\n",
    "print(f\"Testing loss: {test_loss} Accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'models/ResNet_log_TL.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pure ITW training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We already have data loaders, simply setup the same model and training procedures.\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "    #transforms.Normalize(mean, std)\n",
    "  ])\n",
    "test_transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "# setup the model, exact same one as used prior with empty weights\n",
    "\n",
    "model = None\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "if model_type == \"LSTM\":\n",
    "    model = LSTMSpectrogram()\n",
    "elif model_type == \"enet\":\n",
    "    model =  EfficientNetSpectrogram(\"b0\")\n",
    "elif model_type == \"res\":\n",
    "    model = ResNet50Spectrogram()\n",
    "    \n",
    "model = model.to(device)\n",
    "\n",
    "#epochs = 100\n",
    "epochs = 30\n",
    "weight_decay = 5e-4\n",
    "learning_rate = 0.0001\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "if model_type == \"LSTM\":\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay = weight_decay)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max = epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting pure ITW training using the same model type (with weights cleared)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                            | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0] Training Loss: 0.13709653658139279\n",
      "[Epoch 0] Validation Loss: 0.05969329889398068 Accuracy: 0.9798552093169657\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|██▋                                                                              | 1/30 [09:57<4:48:41, 597.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] Training Loss: 0.048527227357211496\n",
      "[Epoch 1] Validation Loss: 0.047622000323026444 Accuracy: 0.9886685552407932\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|█████▍                                                                           | 2/30 [25:49<6:16:02, 805.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2] Training Loss: 0.03595966197987403\n",
      "[Epoch 2] Validation Loss: 0.042623908452806065 Accuracy: 0.9852061693421467\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|████████                                                                         | 3/30 [41:44<6:33:18, 874.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3] Training Loss: 0.025100845364209794\n",
      "[Epoch 3] Validation Loss: 0.021617065165628446 Accuracy: 0.9924457034938622\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|██████████▊                                                                      | 4/30 [57:37<6:32:21, 905.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 4] Training Loss: 0.01808495312562061\n",
      "[Epoch 4] Validation Loss: 0.038589535914488805 Accuracy: 0.9905571293673276\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█████████████▏                                                                 | 5/30 [1:13:33<6:24:46, 923.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 5] Training Loss: 0.01925506910306267\n",
      "[Epoch 5] Validation Loss: 0.01179993757934426 Accuracy: 0.9959080893925086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|███████████████▊                                                               | 6/30 [1:29:26<6:13:25, 933.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 6] Training Loss: 0.014157977252735755\n",
      "[Epoch 6] Validation Loss: 0.023532813157471535 Accuracy: 0.9940195152659742\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██████████████████▍                                                            | 7/30 [1:45:22<6:00:39, 940.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 7] Training Loss: 0.013864283821969492\n",
      "[Epoch 7] Validation Loss: 0.019728451837909233 Accuracy: 0.9933899905571294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|█████████████████████                                                          | 8/30 [2:01:16<5:46:30, 945.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 8] Training Loss: 0.010197309929393463\n",
      "[Epoch 8] Validation Loss: 0.014410113999083478 Accuracy: 0.9959080893925086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███████████████████████▋                                                       | 9/30 [2:17:09<5:31:38, 947.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 9] Training Loss: 0.010863139021807768\n",
      "[Epoch 9] Validation Loss: 0.016034580965015265 Accuracy: 0.9937047529115518\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|██████████████████████████                                                    | 10/30 [2:33:04<5:16:40, 950.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 10] Training Loss: 0.0070711572845911005\n",
      "[Epoch 10] Validation Loss: 0.006737054672780687 Accuracy: 0.9981114258734656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|████████████████████████████▌                                                 | 11/30 [2:48:59<5:01:16, 951.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 11] Training Loss: 0.005095998747590944\n",
      "[Epoch 11] Validation Loss: 0.02038345941118223 Accuracy: 0.9927604658482846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|███████████████████████████████▏                                              | 12/30 [3:04:55<4:45:51, 952.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 12] Training Loss: 0.00656310174268932\n",
      "[Epoch 12] Validation Loss: 0.006841832745883494 Accuracy: 0.9974819011646208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|█████████████████████████████████▊                                            | 13/30 [3:20:52<4:30:18, 954.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 13] Training Loss: 0.002597256415372401\n",
      "[Epoch 13] Validation Loss: 0.00817999015959245 Accuracy: 0.9981114258734656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████████████████████████████████████▍                                         | 14/30 [3:36:48<4:14:35, 954.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 14] Training Loss: 0.0034443782434343847\n",
      "[Epoch 14] Validation Loss: 0.01158743194782801 Accuracy: 0.9965376141013534\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|███████████████████████████████████████                                       | 15/30 [3:52:45<3:58:51, 955.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 15] Training Loss: 0.0031726516899655637\n",
      "[Epoch 15] Validation Loss: 0.012208493231513557 Accuracy: 0.9968523764557758\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|███████████████████████████████████████                                       | 15/30 [4:08:39<4:08:39, 994.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Triggering early breaking on epoch 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Train on the ITW dataset with an empty model\n",
    "\n",
    "# Same loop, but for our ITW transfer learning.\n",
    "\n",
    "patience = 5\n",
    "best_validation_loss = 10000.0\n",
    "fail_count = 0\n",
    "\n",
    "ITW_training_losses = []\n",
    "ITW_val_losses = []\n",
    "# ITW_test_losses = []\n",
    "\n",
    "print(\"Starting pure ITW training using the same model type (with weights cleared)\")\n",
    "\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    training_loss = train(ITW_train_loader)\n",
    "    print(f\"[Epoch {epoch}] Training Loss: {training_loss}\")\n",
    "    \n",
    "    ITW_training_losses.append(training_loss)\n",
    "    \n",
    "    validation_loss, val_accuracy = validate(ITW_val_loader)    \n",
    "    print(f\"[Epoch {epoch}] Validation Loss: {validation_loss} Accuracy: {val_accuracy}\")\n",
    "    \n",
    "    ITW_val_losses.append(validation_loss)\n",
    "    \n",
    "    test_loss, test_accuracy = validate(ITW_test_loader)\n",
    "    # print(f\"[DEBUG Epoch {epoch}] Test Loss: {test_loss} Accuracy: {test_accuracy}\")\n",
    "    \n",
    "    # ITW_test_losses.append(test_loss)\n",
    "    \n",
    "    if validation_loss < best_validation_loss:\n",
    "        best_validation_loss = validation_loss\n",
    "        fail_count = 0\n",
    "    else:\n",
    "        # increment number of epochs of no improvement\n",
    "        fail_count = fail_count + 1\n",
    "        \n",
    "    if fail_count >= patience:\n",
    "        print(f\"Triggering early breaking on epoch {epoch}\")\n",
    "        break\n",
    "    \n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pure ITW Test EER: 0.005125068724934024\n",
      "Testing loss: 0.011843823992901435 Accuracy: 0.9968543567159485\n"
     ]
    }
   ],
   "source": [
    "print(f\"Pure ITW Test EER: {compute_EER(model, ITW_test_loader)}\")\n",
    "test_loss, test_accuracy = validate(ITW_test_loader)\n",
    "print(f\"Testing loss: {test_loss} Accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'models/ResNet50_ITW.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark the inference time of the model (should be same across either training method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average run time for batch of size 32 on model res with features cqt\n",
      "99.91057424545288 ms\n",
      "Averages to 3.1222054451704024 ms per input\n"
     ]
    }
   ],
   "source": [
    "# get sample data\n",
    "input_data, _ = next(iter(ITW_test_loader))\n",
    "input_data = input_data.to(device)\n",
    "\n",
    "# make sure model is in fastest cache\n",
    "with torch.no_grad():\n",
    "    for _ in range(5):\n",
    "        _ = model(input_data)\n",
    "\n",
    "n_bench_runs = 1000\n",
    "run_times = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for _ in range(n_bench_runs):\n",
    "        # important to make sure each run is done sequentially\n",
    "        torch.cuda.synchronize()\n",
    "        \n",
    "        start = torch.cuda.Event(enable_timing = True)\n",
    "        end = torch.cuda.Event(enable_timing = True)\n",
    "        \n",
    "        start.record()\n",
    "        _ = model(input_data)\n",
    "        end.record()\n",
    "        \n",
    "        # important to make sure each run is done sequentially\n",
    "        torch.cuda.synchronize()\n",
    "        \n",
    "        run_times.append(start.elapsed_time(end))\n",
    "    \n",
    "average_rtime = sum(run_times) / n_bench_runs\n",
    "print(f'Average run time for batch of size {batch_size} on model {model_type} with features {feature_type}')\n",
    "print(f'{average_rtime} ms')\n",
    "print(f'Averages to {average_rtime / batch_size} ms per input')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
